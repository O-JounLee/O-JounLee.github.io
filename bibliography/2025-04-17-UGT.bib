@article{Hoang202412456,
	abbr={AAAI},
	dimensions={true},
	selected={true},
	author = {Hoang, Van Thuy and Lee, O-Joun},
	title = {Transitivity-Preserving Graph Representation Learning for Bridging Local Connectivity and Role-Based Similarity},
	year = {2024},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume = {38},
	number = {11},
	pages = {12456 – 12465},
	doi = {10.1609/aaai.v38i11.29138},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183974141&doi=10.1609%2faaai.v38i11.29138&partnerID=40&md5=ac68fce9bfbad3fbe9be3274809cd224},
	abstract = {Graph representation learning (GRL) methods, such as graph neural networks and graph transformer models, have been successfully used to analyze graph-structured data, mainly focusing on node classification and link prediction tasks. However, the existing studies mostly only consider local connectivity while ignoring long-range connectivity and the roles of nodes. In this paper, we propose Unified Graph Transformer Networks (UGT) that effectively integrate local and global structural information into fixed-length vector representations. First, UGT learns local structure by identifying the local substructures and aggregating features of the k-hop neighborhoods of each node. Second, we construct virtual edges, bridging distant nodes with structural similarity to capture the long-range dependencies. Third, UGT learns unified representations through self-attention, encoding structural distance and p-step transition probability between node pairs. Furthermore, we propose a self-supervised learning task that effectively learns transition probability to fuse local and global structural features, which could then be transferred to other downstream tasks. Experimental results on real-world benchmark datasets over various downstream tasks showed that UGT significantly outperformed baselines that consist of state-of-the-art models. In addition, UGT reaches the third-order Weisfeiler-Lehman power to distinguish non-isomorphic graph pairs. Copyright © 2024, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.}
}
