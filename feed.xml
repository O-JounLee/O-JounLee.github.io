<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://o-jounlee.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://o-jounlee.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-09-05T05:56:04+00:00</updated><id>https://o-jounlee.github.io/feed.xml</id><title type="html">blank</title><subtitle>The academic profile of Prof. O-Joun Lee at The Catholic University of Korea </subtitle><entry><title type="html">Structure-Preserving Graph Transformer 모델들에 대한 Survey 연구 회고</title><link href="https://o-jounlee.github.io/blog/2025/Survey-on-GTs/" rel="alternate" type="text/html" title="Structure-Preserving Graph Transformer 모델들에 대한 Survey 연구 회고"/><published>2025-08-01T15:00:00+00:00</published><updated>2025-08-01T15:00:00+00:00</updated><id>https://o-jounlee.github.io/blog/2025/Survey-on-GTs</id><content type="html" xml:base="https://o-jounlee.github.io/blog/2025/Survey-on-GTs/"><![CDATA[<p><br/></p> <div style="display: block; margin-left: auto; margin-right: auto; width:100%; text-align:center;"> <a href="https://arxiv.org/abs/2401.16176" class="btn btn--primary">read the paper</a> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29138" class="btn btn--primary">read UGT paper</a> <a href="https://ieeexplore.ieee.org/document/10974679" class="btn btn--primary">read CGT paper</a> </div> <p><br/></p> <p>이 글에서는 우리 연구실(Network Science Lab @ CUK)의 Van Thuy Hoang 박사과정과 함께 진행한 Structure-Preserving Graph Transformer 모델들에 대한 Survey 연구<d-footnote>Van Thuy Hoang, O-Joun Lee: A Survey on Structure-Preserving Graph Transformers. arXiv preprint 01/2024; arXiv:2401.16176. (Preprint)</d-footnote>의 배경과, 논문에 포함되지 못했던 고민들을 정리하고자 한다. 해당 논문은 IJCAI 2024의 Survey Track에 투고하였으나 아쉽게도 채택되지는 못했다.</p> <h1 id="motivation">Motivation</h1> <p>Van Thuy Hoang 박사과정과 함께 <a href="https://ojs.aaai.org/index.php/AAAI/article/view/29138">UGT</a>와 <a href="https://ieeexplore.ieee.org/document/10974679">CGT</a>에 대한 연구를 진행하면서, Structure-Preserving Graph Transformers의 설계원칙과 Design Space에 대한 우리의 이해를 어느정도 정립할 수 있었다. 앞선 UGT와 CGT 연구를 진행하면서 늘 고민했던 부분은 “Graph Transformer는 그래프의 구조를 얼마나 잘 보존할 수 있을까?”라는 질문이었다. Multi-head Attention을 바탕으로 전체 입력 토큰 간의 정보를 전파하는 Transformer는 Permutation Invariance를 보장할 수 있지만, Over-globalization에 대한 문제를 피하기 힘들다. Graph Transformer 모델들이 지역적/전역적 구조 정보를 최대한 보존하여, 지역적 인접성을 갖거나 구조적 유사성을 갖는 노드들 만이 비슷한 벡터 표현을 갖도록 하는 Structure-Preserving Graph Transformer 모델들에 대한 고민이 필요한 이유이다. 우리는 이를 위한 Graph Transformers의 구조 정보를 주입하기 위한 개선 방향성을 Node Feature Modulation, Context Node Sampling, Graph Rewriting (혹은 Rewiring), Transformer Architecture Improvements의 네 가지로 분류하고 여기에 대한 우리의 생각을 Short Survey 형태로 정리했다.</p> <h1 id="node-feature-modulation">Node Feature Modulation</h1> <p>첫 번째 축은 구조 정보를 초기 노드 특징 벡터에 주입하는 방식으로 자연어처리 분야에서 토큰의 순서나 문장 구분을 Positional encoding을 통해 전달하는 것과 비슷한 접근이다. 대표적인 방법은 Laplacian eigenvector를 Positional encoding으로 사용하는 방식이다. Laplacian eigenvector는 1-hop connectivity에 대한 정보만을 내포한다는 한계을 가짐에도, 많은 연구에서 분자 성질 예측 벤치마크 상에서 SOTA 성능을 보고한 바 있다. 특히, TokenGT의 경우 encoding을 엣지 토큰까지 확장하면 2-WL 수준의 구조 식별력을 달성할 수 있고 구조 정보가 모델 표현력에도 기여한다는 것을 보였다.</p> <p>Laplacian eigenvector 보다 큰 규모의 구조 정보를 나타낼 수 있는 Random walk, Transition probability, Degree sequence, Shortest path distance (SPD) 기반의 방법들 또한 자주 사용된다. SPD의 경우에는 전역적 구조 상에서 목표 노드의 상대적 위치를 표현하는데 장점이 있으며, 나머지 특징들은 Multi-scale로 목표 노드 주변의 구조를 표현하는데 장점을 갖는다. 특히, 보고된 실험 결과들에 따르면 Heterophily 특성이 강한 그래프에서는 단순한 Eigenvector positional encoding보다는 Random walk 기반의 Positional encoding이 더 일관된 예측을 보인다. 이외에도 Graph kernel로 많이 활용되는 다양한 구조적 특징들이 Positional encoding, Initial residual, Attention score 등을 통해 모델에 추가적인 입력으로 제공되거나, Graph augmentation이나 Node sampling에 활용된다.</p> <h1 id="context-node-sampling">Context Node Sampling</h1> <p>두 번째 축은 Graph Transformer의 Receptive filed, 즉, 목표 노드의 Context 노드 집합을 어떻게 정의할지에 대한 문제다. 모든 입력 토큰 사이에 정보를 전파하는 Transformer는 노드 간 인접성 정보를 훼손할 수 있으며, 일반적으로 1-hop 이웃을 Context의 범위로 하는 Graph Neural Network 모델들은 Long-range Dependecy 문제와 여기서 이어지는 Over-smoothing이나 Over-squashing 문제들을 마주하기 마련이다. 이 문제를 해소하기 위해서는 전체 노드와 1-hop 이웃 사이의 어느 지점으로 Context 노드의 범위를 조정할 필요가 있으며, 이때 가장 크게 고려되어야 할 사항은 어떤 노드들이 충분히 비슷한 노드들인가 그리고 어떤 노드들에게 비슷한 벡터 표현이 주어져야 할지이다.</p> <p>지역적인 샘플링을 사용하는 Graphformer나 EGT, Gophormer 같은 모델들은 주로 k-hop 이웃이나 ego-network를 활용해 Context의 범위를 제한한다. 반면 DGT는 노드 특징 유사도를, UGT는 노드의 구조적 유사도를 기반으로 하는 전역적인 Context 노드 샘플링을 함께 이용하는 방식을 취한다. 보고된 실험 결과들을 살펴보면 Heterophily 특성이 강한 그래프에서는 구조적 유사도 기반 샘플링이 노드 특징 유사도 기반의 방식보다 더 안정적이고 일반화 성능을 보였다. 이런 결과들은 구조적으로 의미 있는 정보는 지역적 연결성만이 아니라 전역적인 Context에서 비롯된다는 점을 뒷받침한다.</p> <h1 id="graph-rewriting">Graph Rewriting</h1> <p>세 번째 축은 그래프 자체를 재구성하거나 변형하여 구조 특징을 반영하고 Attention score의 계산 범위를 조정하는 방식이다. 노드 샘플링을 바탕으로 하는 방법들이 목표 노드와 연결성이나 구조적 유사성을 갖는 노드들을 Context 노드 집합에 포함시키는 것과 달리, 그래프 재구성의 경우 이런 노드들을 목표 노드와 연결하는 (Virtual) 엣지를 추가한다.</p> <p>GT나 SAN처럼 Context 노드 집합 내의 모든 노드를 완전히 연결하는 방식은 작은 그래프에서는 유효하지만, 노드 수가 수만 개 단위에 이르면 메모리 비용이 지나치게 증가한다. 그래프 Coarsening을 도입한 Coarformer나 ANS‑GT는 Super 노드를 이용해 복잡도를 줄이지만, 이 역시 그래프 Partitioning 방법과 데이터 특성에 따라 성능 차이가 크다. 이에 비해, UGT에서 채택한 Virtual 엣지 추가는 구조적으로 유사한 노드 쌍을 연결하는 새로운 엣지를 추가하는 간단한 방식만으로도 Heterophily 환경에서 유의미한 성능 향상을 얻을 수 있다는 장점이 있다. 하지만, 추가할 Virtual 엣지의 수를 유동적으로 조절하지 못한다는 것이 실제 그래프의 구조에 따라서는 Noisy Signal 문제를 일으킴이 실험적으로 활용되기도 했다. 이런 문제들을 해결하기 위한 Adaptive sparsification에 대한 연구가 필요한 이유이다.</p> <h1 id="architecture-improvements">Architecture Improvements</h1> <p>네 번째 축은 Transformer 자체의 구조 및 Attention Mechanism을 개선하여 구조 정보를 학습하도록 하는 방향이다. 구조 식별력은 구조적으로 다른 노드 쌍 혹은 그래프 쌍에 대해 식별 가능한 벡터 표현을 부여하느냐에 대한 문제로, GIN 등의 널리 알려진 모델들을 통해 논의된 바와 같이 모델 자체가 구조와 벡터 표현을 1:1 맵핑할 수 있을 정도의 표현력을 갖느냐하는 문제가 가장 주요하다. 하지만, UGT와 CGT에서 이용된 것처럼, Initial residual connection을 통해 구조적 특징 벡터를 전달하거나 Attention score가 구조적 유사도를 반영할 수 있도록 한다면, 모델 Efficiency와 구조 표현력을 동시에 확보하는데 도움이 된다.</p> <p>Attention score를 보강하는데는 구조적 유사도에 대한 항을 추가해주거나 노드 특징 행렬에 구조적 특징을 추가하는 방식이 많이 활용된다. 구조적 특징으로는 Node Feature Modulation의 경우와 같이 Graph Kernel에서 부터 이용되던 다양한 구조 정보들이 활용된다. UGT와 CGT는 타겟 노드의 주변 구조에 대한 Description 능력이 좋은 Degree Sequence나 SPD 같은 정보들을 High-order 인접성 정보를 잘 나타낼 수 있는 Transition probability 등과 함께 활용하였는데, 이는 구조적 유사도만 이용할 경우 Attention이 그래프 전역에 분산될 수 있으므로 High-order 인접성을 바탕으로 더 의미있는 Sparse attention을 형성하고자 하는 목적을 가지고 있다.</p> <p>Graph Transformer에 대한 비교적 초창기 연구들 중에는 Transformer 레이어 이전에 GNN 레이어를 쌓아 전역적인 Self-attention 적용으로 인한 구조 정보 손실을 보완하려는 시도들이 있었다. Transformer와 GNN 구조 간의 Context 범위 및 정보전파 범위 차이는 둘이 병용될 경우 Over-globalization과 Long-range Dependency 문제 등을 어느정도 완화하지만, 근본적인 해결책이 되기에는 어려움이 있다.</p> <h1 id="conclusion">Conclusion</h1> <p>CGT에 대한 연구를 마무리하고 IJCAI 2024에 투고하기 위한 작업을 하면서, 막간의 여유를 이용해 이 Short Survey 또한 IJCAI 2024의 Survey Track에 투고하기 위해 준비했다. 두 논문 모두 좋은 성과를 거두지는 못한 지금 되돌아보자면, CGT에 대한 논문을 더 다듬는데 시간과 에너지를 더 투자하고 Graph Transformer의 Design space에 대한 논의는 조금더 시간을 두고 우리의 생각을 다듬었다면 좋은 성과가 있지 않았을까 생각한다.</p> <p>이 Short Survey 작업은 Structure preservation 관점에서 Graph Transformer의 Design space에 대한 단순한 정리를 넘어, 각 설계 요소들이 서로 어떻게 영향을 주고받는지를 보다 체계적으로 조망할 수 있었다. 네 가지 축은 각각 모델의 표현력, 확장성, 일반화 능력에 영향을 주며, Graph Transformer를 설계하는 데 있어 반드시 함께 고려되어야 할 요소들임을 다시금 확인할 수 있었다.</p>]]></content><author><name>O-Joun Lee</name></author><category term="Research"/><category term="GNNs"/><category term="GTs"/><category term="GRL"/><category term="GraphML"/><category term="Papers"/><summary type="html"><![CDATA[Graph Transformer의 Design Space에 대한 고민들]]></summary></entry><entry><title type="html">Major of AI-Chemical Engineering Launched</title><link href="https://o-jounlee.github.io/blog/2025/major-of-ai-chemical-engineering-launched/" rel="alternate" type="text/html" title="Major of AI-Chemical Engineering Launched"/><published>2025-05-26T15:00:00+00:00</published><updated>2025-05-26T15:00:00+00:00</updated><id>https://o-jounlee.github.io/blog/2025/major-of-ai-chemical-engineering-launched</id><content type="html" xml:base="https://o-jounlee.github.io/blog/2025/major-of-ai-chemical-engineering-launched/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We are pleased to announce the establishment of a new Major of AI-Chemical Engineering in the Department of Artificial Intelligence at the Graduate School, in addition to the existing Major of Artificial Intelligence.]]></summary></entry><entry><title type="html">Community-aware Graph Transformer (CGT) 연구에 대한 회고</title><link href="https://o-jounlee.github.io/blog/2025/CGT/" rel="alternate" type="text/html" title="Community-aware Graph Transformer (CGT) 연구에 대한 회고"/><published>2025-05-11T15:00:00+00:00</published><updated>2025-05-11T15:00:00+00:00</updated><id>https://o-jounlee.github.io/blog/2025/CGT</id><content type="html" xml:base="https://o-jounlee.github.io/blog/2025/CGT/"><![CDATA[<p><br/></p> <div style="display: block; margin-left: auto; margin-right: auto; width:100%; text-align:center;"> <a href="https://doi.org/10.1109/TNSE.2025.3563697" class="btn btn--primary">read the paper</a> <a href="https://nslab-cuk.github.io/2023/12/27/CGT/" class="btn btn--primary">read Thuy's Post</a> <a href="https://github.com/NSLab-CUK/Community-aware-Graph-Transformer" class="btn btn--primary">get the code</a> </div> <p><br/></p> <p>이 글에서는 우리 연구실(<a href="https://nslab-cuk.github.io/">Network Science Lab @ CUK</a>)의 <a href="https://nslab-cuk.github.io/member/hoangvanthuy90">Van Thuy Hoang</a> 박사과정과 함께 진행하여 IEEE TSNE에 게재한 Community-aware Graph Transformer(CGT) 연구<d-footnote>Van Thuy Hoang, Hyeon-Ju Jeon, O-Joun Lee: Mitigating Degree Bias in Graph Representation Learning with Learnable Structural Augmentation and Structural Self-attention. IEEE Transactions on Network Science and Engineering 04/2025. DOI:10.1109/TNSE.2025.3563697</d-footnote>의 배경과 설계 원칙, 그리고 논문에는 포함되지 못했던 근본적인 개념들, 그리고 다사다난했던 연구 과정에 대해 회고하고자 한다.</p> <h1 id="a-remaining-problem-in-disassortative-and-heterophilic-graphs">A Remaining Problem in Disassortative and Heterophilic Graphs</h1> <p><a href="https://doi.org/10.1609/aaai.v38i11.29138">Unified Graph Transformer</a> 연구를 통해 우리는 원본 그래프의 구조적 특징을 보존하면서도 인접하지 않으나 높은 Structural Similarity를 갖는 노드들 사이에 Message를 전파할 방법을 제시했다. 이 방법은 다양한 Downstream Task에서 SOTA 성능을 보였을 뿐만 아니라, Node Clustering과 Graph Classification에 대한 SOTA 성능 및 3d-WL에 준하는 Isomorphism Testing 성능을 통해 구조적 특징 보존 능력을 입증했다. 하지만, k-step Transition Probability와 k-hop Proximity를 바탕으로 구조적 유사도가 높은 노드 사이에 Message 전파를 강화하는 이 방식은 Dissortative하고 Heterophilic한 실세계 그래프들에서 흔하게 발생하는 고질적인 문제인 Degree Bias 문제를 해결하는 것에는 그리 효과가 없었다.</p> <h1 id="causes-of-degree-bias-in-graph-theory-view">Causes of Degree Bias in Graph Theory View</h1> <p>Degree Bias의 주된 발생 원인은 Degree에 따라 노드가 수신하는 Message의 양적 그리고 질적 격차가 발생하는 것인데, 이에 따라 Degree가 낮은 노드와 높은 노드 모두에서 노드 Representation의 품질 저하와 Task 성능 저하 문제가 발생하는 것을 확인할 수 있었다. 실세계 그래프 데이터의 상당 부분을 차지하는 Scale-free 그래프들의 노드 Degree 분포가 멱함수 분포를 따른다는 점은 Downstream Task에서의 평균적인 성능 저하를 더욱 심화시켰다. 이와 같은 문제의 근본적인 원인은 Node Homophily에 대한 가정을 바탕으로 인접 노드로부터 전달되는 Message를 취합하여 목표 노드의 Representation을 업데이트하는 Message Passing Mechanism의 특성에 있다. 인접한 노드는 비슷한 특징을 가질 것이라는 이 가정은 Degree가 높은 노드는 지나치게 많은 Message로 인해 Over-Representation되도록 하며 Degree가 낮은 노드는 지나치게 적은 Message로 인해 Under-Representation되게 만든다. 이 문제는 Scale-free 그래프의 구조적 특징으로 인해 실세계 그래프들에서 더욱 심각해지는데, 소수의 Degree가 높은 노드(허브)를 중심으로 대다수의 Degree가 낮은 노드(단말)들이 Community를 이루며 모여 있고 Community 사이의 연결은 Degree가 높은 노드들이 직접 연결되거나 소수의 매개자 역할을 하는 노드를 통하기 때문이다. 이러한 구조로 인해, Degree가 높은 노드들, 즉 Community의 허브 노드들은 많은 양의 Message들을 받더라도 대부분의 Message가 구조적으로 이질적인 동일 Community의 말단 노드로부터 오거나 구조적으로는 동질적이더라도 Proximity가 낮은 다른 Community의 허브 노드로부터 오기 때문에 수신되는 Message들의 막대한 양에 비하여 대부분의 Message가 Noisy할 수밖에 없다. 마찬가지로 Degree가 낮은 노드 중 대다수를 차지하는 Community의 단말 노드들도 소수의 Message가 거의 모두 구조적으로 이질적인 동일 Community의 허브 노드로부터 온다는 문제가 있다.</p> <h1 id="resolving-degree-bias-by-utilizing-community-structures">Resolving Degree Bias by Utilizing Community Structures</h1> <p>여기서 우리의 가장 큰 아이디어는 Community 구조로부터 오는 문제를 Community 구조를 활용해서 해결하는 것이었다. 커뮤니티는 소속된 노드 간에는 높은 Proximity를 외부 노드에 대해서는 낮은 Proximity를 갖는, 즉 Node Homophily에 대한 가정을 따르면 내부적으로는 동질적인 특성을 외부에 대해서는 배타적인 특성을 갖는 노드들의 집단이다. 이에 따라, 동일 커뮤니티에 속한 노드들은 일정 수준 이상의 High-order Proximity를 가지고 있으며 Informative Message의 Source로 고려할 첫 번째 대상이라고 볼 수 있다. 하지만, 앞서 논의한 바와 같이 하나의 Community에도 허브 노드와 단말 노드가 존재하며, 구조적으로 이질적인 이들이 비슷한 특징을 가질 것으로 기대하기는 어렵다. 이에 따라, High-order Proximity와 Structural Similarity를 함께 갖춘 노드라야 서로의 Representation을 업데이트하는데 유의미한 정보를 전달할 수 있을 것으로 보고, 우리는 동일한 Community에 속하면서도 구조적으로 동질적인 노드 사이에 Message가 전파되도록 하면 노드 Representation의 질을 개선하고 Degree Bias 문제를 해결하면서도 Message 전파의 경로를 수정함으로써 발생하는 구조적 특징 정보 손실을 최소화할 수 있을 것이라 보았다.</p> <h1 id="learnable-structure-augmentation-and-community-aware-graph-transformer">Learnable Structure Augmentation and Community-aware Graph Transformer</h1> <p>위의 원칙을 바탕으로 한 그래프 구조의 Augmentation을 위하여 우리는 Edge Perturbation 기반의 Structure Learning을 채용했다. 일반적으로 노드 특징의 연관성을 바탕으로 하는 Edge Perturbation에 Community에 대한 Membership과 High-order Proximity, Role-based Similarity를 추가로 고려할 수 있도록 하였으며, 이 특징들은 Transformer Layer의 Attention Score에서도 같은 방식으로 고려되었다. 추가로, 제안하는 방법이 노드의 고차 인접성과 구조적 동질성을 모두 고려한다고 하더라도, Structure Learning을 통한 그래프 구조의 변화는 그래프 구조적 특징 정보의 소실을 동반할 수밖에 없다. 이를 보완하기 위해, 우리는 Structure Learning에 대한 Regularization으로 UGT 모델에서도 활용했던 Transition Probability의 보존 임무를 함께 채용했다. 이를 통해, 모델은 Task 수행에 최적화된 방향으로 커뮤니티 구조 및 각 노드가 커뮤니티 내에서 갖는 역할을 고려하여 간선을 추가하거나 삭제하면서 Informative Message를 최대화하고 Noisy Message를 최소화하지만, 이러한 구조의 변경을 고차 인접성 정보인 Transition Probability를 재건할 수 있는 선에서 수행하도록 학습된다.</p> <p>이 모델의 평가를 위해, 우리는 노드 Degree에 따른 모델의 성능 변화를 확인하였고, 제안하는 모델이 기존 방법들 대비 Degree 편향을 가장 유의미하게 완화할 수 있는 방법 중 하나라는 것을 확인하였다. 이와 동시에 제안한 모델인 CGT는 전체적인 노드 분류 성능과 군집화 성능에서도 최고 수준을 보였으며, 특히 군집화 성능은 제안한 모델이 구조 정보의 소실을 최소화했음을 방증하기도 한다. 추가로, 우리는 실험 결과를 분석하면서 모델의 전반적인 성능과 Degree 편향 완화 사이에 Trade-off 관계가 있음을 발견하였는데, 이러한 면에서도 제안한 모델은 가장 약한 수준의 Trade-off를 보였다. 결론적으로, 우리는 실세계 그래프 데이터들의 대다수를 차지하는 Scale-free 그래프에서 Message Passing을 적용할 경우 Degree 편향이 발생하는 주된 이유가 Community 구조에서 온다고 봤고, 이에 따라 Community Membership과 노드들의 Community 내 역할을 고려할 수 있는 Learnable Structure Augmentation 방법과 Graph Transformer 구조를 제시하여 해당 문제를 유의미하게 개선할 수 있었다.</p> <h1 id="rigorousness-venue-selection-writing-style-and-currentness">Rigorousness, Venue Selection, Writing Style, and Currentness</h1> <p>앞선 Unified Graph Transformer(UGT)에 대한 연구가 예상 밖의 뛰어난 성과를 거두고 나서, 우리는 우리의 연구를 좀 더 꼼꼼하게 그리고 단단하게 다지는 과정에 충분한 노력을 기울이지 않았던 것 같다. Community-aware Graph Transformer(CGT)에 대한 실험과 검증을 하던 시점에서 CGT는 우리의 앞선 모델인 UGT와 비교해서도 대부분의 Node-level 그리고 Graph-level Task들에서 우수한 SOTA 수준의 성능을 보였다. 하지만, 우리는 제안된 모델의 가장 주요한 설계 목표인 Degree Bias에 대한 뚜렷하고 객관적인 검증 및 평가 방법을 제시해야 했으나 그렇지 못했다. 2023년 겨울, 이 연구를 막 마무리한 시점에서 Degree 범위에 따른 모델 성능 편차를 확인하는 것은 우리가 채용할 수 있는 최선의 방법 중 하나라고 생각했으나 곧 다른 연구에서 DEO(Degree Equal Opportunity)와 DSP(Degree Statistical Parity) 등과 같은 좀 더 체계적인 평가 방법이 제시되었다. 이러한 검증과 평가 면에서의 엄밀함의 부족이 처음 투고했던 IJCAI 2024에서 좋은 결과를 얻지 못한 원인이었다고 생각된다. 그 후, 우리는 대부분의 실험을 다시 설계하고 실험의 폭 또한 대폭 확장하여 CIKM 2024에 투고하였으나, 처음 논문을 투고할 때는 꽤 신선한 접근이었던 Structure Learning을 바탕으로 한 그래프 증강은 이 시점에 와서는 참신성이 부족했고, 무엇보다 우리가 Data Mining 분야 컨퍼런스가 요구하는 Writing Style에 익숙하지 않았다. 이 시기마저 놓치고 나니 다른 컨퍼런스들에서는 2023년 겨울에 이미 arXiv와 Github에 공개된 우리 논문과 비슷한 접근과 방법을 채용한 논문들이 눈에 띄기 시작했고, AAAI 2025에서마저도 좋은 결과를 얻을 수 없었기는 마찬가지였다. 다행스럽게도 우리가 진행했던 폭넓은 실험에 대해서 Discussion의 폭과 깊이를 크게 보완한 끝에 유의미한 Insight들을 잘 정리하여 네트워크 과학 분야의 권위 있는 학술지 중 하나인 IEEE Transactions on Network Science and Engineering(TNSE)에 연구를 발표하게 되어 좋은 결과로 마무리할 수 있었다. 돌이켜 보면 이 일련의 과정은 PI로써 미숙한 부분이 많았던 나의 연구 기획 역량의 부족이었던 것으로 생각된다. Van Thuy Hoang 박사과정이 수행한 깊이 있는 연구가 곧바로 좋은 결과로 이어지지 못하고 여러 번 고배를 마시게 된 것에는 아직도 미안함이 크게 남는다. 부가적으로, 추후 여러 논문심사 과정을 거치며 현재 모델의 이름은 DegFairGT로 수정되었다.</p>]]></content><author><name>O-Joun Lee</name></author><category term="Research"/><category term="GNNs"/><category term="GTs"/><category term="GRL"/><category term="GraphML"/><category term="Papers"/><summary type="html"><![CDATA[Community 구조를 활용한 Disassortative 및 Heterophilic 그래프 핸들링에 대한 고민들]]></summary></entry><entry><title type="html">One NS-CUK paper accepted for IEEE TNSE</title><link href="https://o-jounlee.github.io/blog/2025/one-ns-cuk-paper-accepted-for-ieee-tnse/" rel="alternate" type="text/html" title="One NS-CUK paper accepted for IEEE TNSE"/><published>2025-04-18T15:00:00+00:00</published><updated>2025-04-18T15:00:00+00:00</updated><id>https://o-jounlee.github.io/blog/2025/one-ns-cuk-paper-accepted-for-ieee-tnse</id><content type="html" xml:base="https://o-jounlee.github.io/blog/2025/one-ns-cuk-paper-accepted-for-ieee-tnse/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Members of the Network Science Lab at the Catholic University of Korea submitted one paper on mitigating the degree bias problem in the message passing mechanism to the IEEE TNSE (Transactions on Network Science and Engineering) and was accepted.]]></summary></entry><entry><title type="html">Unified Graph Transformer (UGT) 연구에 대한 회고</title><link href="https://o-jounlee.github.io/blog/2025/UGT/" rel="alternate" type="text/html" title="Unified Graph Transformer (UGT) 연구에 대한 회고"/><published>2025-04-16T15:00:00+00:00</published><updated>2025-04-16T15:00:00+00:00</updated><id>https://o-jounlee.github.io/blog/2025/UGT</id><content type="html" xml:base="https://o-jounlee.github.io/blog/2025/UGT/"><![CDATA[<p><br/></p> <div style="display: block; margin-left: auto; margin-right: auto; width:100%; text-align:center;"> <a href="https://doi.org/10.1609/aaai.v38i11.29138" class="btn btn--primary">read the paper</a> <a href="https://nslab-cuk.github.io/2023/08/17/UGT/" class="btn btn--primary">read Thuy's Post</a> <a href="https://github.com/NSLab-CUK/Unified-Graph-Transformer" class="btn btn--primary">get the code</a> </div> <p><br/></p> <p>이 글에서는 우리 연구실(<a href="https://nslab-cuk.github.io/">Network Science Lab @ CUK</a>)의 <a href="https://nslab-cuk.github.io/member/hoangvanthuy90">Van Thuy Hoang</a> 박사과정과 함께 진행하여 AAAI 2024에서 발표한 Unified Graph Transformer (UGT) 연구<d-footnote>Van Thuy Hoang, O-Joun Lee: Transitivity-Preserving Graph Representation Learning for Bridging Local Connectivity and Role-based Similarity. The 38th AAAI Conference on Artificial Intelligence (AAAI 2024), Vancouver, Canada; 02/2024. DOI:10.1609/aaai.v38i11.29138</d-footnote>의 배경과 설계 원칙, 그리고 논문에는 포함되지 못했던 근본적인 개념들에 대해 회고하고자 한다.</p> <h1 id="motivation">Motivation</h1> <p>우리 연구는 기존 Graph Neural Networks (GNNs)와 Graph Transformers (GTs)의 주된 작동 원리인 Node Homophily 기반의 Message Passing Mechanism이 실제로 Disassortative하거나 Heterophilic한 현실 그래프 환경에서도 효과적으로 작동할 수 있는지에 대한 의문에서 시작되었다. 특히, 우리가 중점적으로 다룬 문제는 비슷한 구조적 특성을 가진 노드들이 유사한 벡터 표현(Vector Representation)을 갖도록 하는 Structural Distinctiveness 문제였다. 기존의 연구들은 주로 구조적으로 유사한 노드 사이에 Pseudo Edge를 추가하는 방법을 제안해왔지만, 우리는 이러한 방식이 오히려 원본 그래프의 구조를 왜곡할 위험이 있다고 판단했다. 즉, 추가된 Edge는 구조적 유사성을 반영하기 위한 것이지만, 실제로는 Graph Encoder의 구조적 식별력(structural discriminability)을 오히려 저하시킬 가능성이 있었다.</p> <h1 id="bridging-local-connectivity-and-structural-similarity">Bridging Local Connectivity and Structural Similarity</h1> <p>우리는 이 문제의 본질적인 해법을 Local Connectivity 정보와 Structural Similarity 정보를 동시에 잘 반영하는 접근법에서 찾아야 한다고 생각했다. 이를 위해 제안한 개념이 바로 k-step Transition Probability이다. k-step Transition Probability는 두 노드 사이에 k-hop 이내의 모든 경로 정보를 반영하는 개념으로, 이는 경로의 개수뿐 아니라 노드들의 Degree에 따른 정보 전파의 확률과 감쇄 효과를 포함하여 주변의 구조적 특성을 자연스럽게 표현한다. 따라서 우리는 이 k-step Transition Probability를 보존하는 것을 목표로 한 Pre-Training Task를 설계하여, Local Connectivity와 Structural Similarity 사이의 근본적인 격차를 효과적으로 줄일 수 있었다.</p> <h1 id="injecting-structural-features">Injecting Structural Features</h1> <p>구조적 식별력 측면에서는 기존의 Graph Isomorphism Network (GIN)이 Sum Aggregation과 Multi-layer Perceptron (MLP)을 이용해 1-dimensional Weisfeiler-Lehman (1d-WL) 테스트 수준의 구조 식별력을 제공하는 것으로 알려져 있다. 그러나 분자 구조 분석과 같은 보다 정교한 구조 식별력이 요구되는 도메인에서는 더 높은 수준의 식별력이 필요하다. 물론 Subgraph Sampling 기반의 GNN과 GT 모델들이 2d-WL 수준의 식별력을 달성할 수 있지만, 이러한 모델들은 k-hop Subgraph Sampling과 Self-Attention의 결합 과정에서 일부 구조 정보를 잃어버릴 수 있는 단점이 존재한다. 기존 연구에서는 이를 해결하기 위해 Laplacian Eigenvector나 Multi-hop Ordered Degree Sequence와 같은 추가적인 Structural Feature를 Initial Node Representation이나 Attention Score에 주입하는 방법을 제안한 바 있다. UGT는 이러한 접근을 한층 발전시켜, Laplacian Eigenvector와 Multi-hop Ordered Degree Sequence, 그리고 앞서 언급한 k-step Transition Probability 정보를 Initial Node Representation과 Attention Score, 그리고 Initial Residual에 결합하여 구조 식별력을 한층 더 향상시켰다.</p> <h1 id="achieving-expressive-power-as-3d-wl">Achieving Expressive Power as 3d-WL</h1> <p>이러한 설계를 통해 UGT는 기존 Structural Feature Injection 기법 및 Structural Similarity를 고려한 Graph Augmentation 기법과 k-step Transition Probability 보존 Task 간에 뛰어난 시너지를 창출하였다. 실제로 Node Classification, Node Clustering, Graph Classification 등 다양한 Downstream Task에서 State-of-the-Art (SOTA) 성능을 기록하였으며, 특히 Heterophilic 그래프와 Homophilic 그래프 모두에서 뛰어난 성능을 보였다는 점에서 우리의 설계 목표를 충분히 달성했다고 판단한다. 또한 Graph Isomorphism Testing에서 3d-WL에 준하는 수준의 구조 식별력을 달성했다는 점은 UGT가 기존 모델 대비 구조 식별력 측면에서 분명한 진보를 이루었음을 입증한 결과였다.</p>]]></content><author><name>O-Joun Lee</name></author><category term="Research"/><category term="GNNs"/><category term="GTs"/><category term="GRL"/><category term="GraphML"/><category term="Papers"/><summary type="html"><![CDATA[Disassortative 및 Heterophilic 그래프 핸들링에 대한 고민들]]></summary></entry><entry><title type="html">Van Thuy Hoang won the Best Award in the 22nd Graduate School Academic Awards</title><link href="https://o-jounlee.github.io/blog/2025/van-thuy-hoang-won-the-best-award-in-the-22nd-graduate-school-academic-awards/" rel="alternate" type="text/html" title="Van Thuy Hoang won the Best Award in the 22nd Graduate School Academic Awards"/><published>2025-03-10T03:00:00+00:00</published><updated>2025-03-10T03:00:00+00:00</updated><id>https://o-jounlee.github.io/blog/2025/van-thuy-hoang-won-the-best-award-in-the-22nd-graduate-school-academic-awards</id><content type="html" xml:base="https://o-jounlee.github.io/blog/2025/van-thuy-hoang-won-the-best-award-in-the-22nd-graduate-school-academic-awards/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Van Thuy Hoang, a doctoral student at the Network Science Lab @ CUK, received the Best Award (최우수상) at the 22nd Graduate School Academic Awards (제22회 대학원 학술상) Ceremony for his outstanding research in graph learning, highlighting the lab's leadership in graph ML.]]></summary></entry><entry><title type="html">One NS-CUK paper presented at AAAI’25</title><link href="https://o-jounlee.github.io/blog/2025/one-ns-cuk-paper-presented-at-aaai25/" rel="alternate" type="text/html" title="One NS-CUK paper presented at AAAI’25"/><published>2025-02-28T09:00:00+00:00</published><updated>2025-02-28T09:00:00+00:00</updated><id>https://o-jounlee.github.io/blog/2025/one-ns-cuk-paper-presented-at-aaai25</id><content type="html" xml:base="https://o-jounlee.github.io/blog/2025/one-ns-cuk-paper-presented-at-aaai25/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Members of the Network Science Lab at the Catholic University of Korea presented a paper on pre-training molecular graph neural networks at the 39th AAAI Conference on Artificial Intelligence (AAAI 2025).]]></summary></entry><entry><title type="html">Halal or Not, A Novel GNN Model for Predicting Cultural Appropriateness of Everyday Products</title><link href="https://o-jounlee.github.io/blog/2025/halal-or-not-a-novel-gnn-model-for-predicting-cultural-appropriateness-of-everyday-products/" rel="alternate" type="text/html" title="Halal or Not, A Novel GNN Model for Predicting Cultural Appropriateness of Everyday Products"/><published>2025-01-08T15:00:00+00:00</published><updated>2025-01-08T15:00:00+00:00</updated><id>https://o-jounlee.github.io/blog/2025/halal-or-not-a-novel-gnn-model-for-predicting-cultural-appropriateness-of-everyday-products</id><content type="html" xml:base="https://o-jounlee.github.io/blog/2025/halal-or-not-a-novel-gnn-model-for-predicting-cultural-appropriateness-of-everyday-products/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[The Network Science Lab at the Catholic University of Korea releases Halal or Not (HaCKG), a novel attributed knowledge graph completion model for predicting cultural appropriateness of everyday products (mainly, halal and haram of cosmetics).]]></summary></entry><entry><title type="html">S-CGIB, A Novel Pre-trained Graph Neural Network Model in Molecular Structure Learning</title><link href="https://o-jounlee.github.io/blog/2024/s-cgib-a-novel-pre-trained-graph-neural-network-model-in-molecular-structure-learning/" rel="alternate" type="text/html" title="S-CGIB, A Novel Pre-trained Graph Neural Network Model in Molecular Structure Learning"/><published>2024-12-19T15:00:00+00:00</published><updated>2024-12-19T15:00:00+00:00</updated><id>https://o-jounlee.github.io/blog/2024/s-cgib-a-novel-pre-trained-graph-neural-network-model-in-molecular-structure-learning</id><content type="html" xml:base="https://o-jounlee.github.io/blog/2024/s-cgib-a-novel-pre-trained-graph-neural-network-model-in-molecular-structure-learning/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[The Network Science Lab at the Catholic University of Korea releases S-CGIB, a novel pre-trained graph neural network model for molecular structure learning.]]></summary></entry><entry><title type="html">One NS-CUK paper accepted for AAAI’25</title><link href="https://o-jounlee.github.io/blog/2024/one-ns-cuk-paper-accepted-for-aaai25/" rel="alternate" type="text/html" title="One NS-CUK paper accepted for AAAI’25"/><published>2024-12-09T15:00:00+00:00</published><updated>2024-12-09T15:00:00+00:00</updated><id>https://o-jounlee.github.io/blog/2024/one-ns-cuk-paper-accepted-for-aaai25</id><content type="html" xml:base="https://o-jounlee.github.io/blog/2024/one-ns-cuk-paper-accepted-for-aaai25/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Members of the Network Science Lab at the Catholic University of Korea submitted one paper on pre-training graph neural networks on molecules to the 39th AAAI Conference on Artificial Intelligence (AAAI 2025) and was accepted.]]></summary></entry></feed>