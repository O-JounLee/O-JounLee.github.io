<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> blog - page 5 | O-Joun Lee </title> <meta name="author" content="O-Joun Lee"> <meta name="description" content="The academic profile of Prof. O-Joun Lee at The Catholic University of Korea "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <meta property="og:site_name" content="O-Joun Lee"> <meta property="og:type" content="article"> <meta property="og:title" content="O-Joun Lee | blog - page 5"> <meta property="og:url" content="https://o-jounlee.github.io/blog/page/5/"> <meta property="og:description" content="The academic profile of Prof. O-Joun Lee at The Catholic University of Korea "> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="blog - page 5"> <meta name="twitter:description" content="The academic profile of Prof. O-Joun Lee at The Catholic University of Korea "> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "O-Joun Lee"
        },
        "url": "https://o-jounlee.github.io/blog/page/5/index.html",
        "@type": "BlogPosting",
        "description": "The academic profile of Prof. O-Joun Lee at The Catholic University of Korea
",
        "headline": "blog - page 5",
        
        "sameAs": ["https://orcid.org/0000-0001-8921-5443", "https://scholar.google.com/citations?user=43GFjoYAAAAJ", "https://www.semanticscholar.org/author/144914730", "https://www.scopus.com/authid/detail.uri?authorId=57150108800", "https://github.com/NSLab-CUK", "https://nslab-cuk.github.io/", "https://dblp.uni-trier.de/pid/160/1579.html", "https://www.kaggle.com/cuknslab"],
        
        "name": "O-Joun Lee",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8E%93&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://o-jounlee.github.io/blog/page/5/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">O-Joun</span> Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <div class="header-bar"> <h1>O-Joun Lee</h1> <h2>Graphs illustrate intricate patterns in our perception of the world and ourselves; graph mining enhances this comprehension by highlighting overlooked details.</h2> </div> <div class="tag-category-list"> <ul class="p-0 m-0"> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/formatting">formatting</a> </li> <p>â€¢</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/images">images</a> </li> <p>â€¢</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/links">links</a> </li> <p>â€¢</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/math">math</a> </li> <p>â€¢</p> <li> <i class="fa-solid fa-hashtag fa-sm"></i> <a href="/blog/tag/code">code</a> </li> <p>â€¢</p> <li> <i class="fa-solid fa-tag fa-sm"></i> <a href="/blog/category/blockquotes">blockquotes</a> </li> </ul> </div> <ul class="post-list"> <li> <h3> <a class="post-title" href="https://nslab-cuk.github.io/2023/11/21/WS/" target="_blank" rel="external nofollow noopener">NS-CUK Winter School on AI Ethics 2023</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote> <p>The 2nd NS-CUK Winter School on Ethics of AI and Robots</p> </blockquote> <blockquote> <p>(NS-CUK AI Ethics 2023; 2023ë…„ ê°€í†¨ë¦­ëŒ€í•™êµ ì¸ê³µì§€ëŠ¥ ìœ¤ë¦¬ ê²¨ìš¸í•™êµ)</p> </blockquote> <hr> <p>NS Lab@CUK organizes a winter school dedicated to the ethics of AI and robots. This program consists of 9 lectures presented by Dr. Eun-Soon You (ìœ ì€ìˆœ ë°•ì‚¬), and is scheduled to take place over a period of two weeks, from December 18th, 2023, to December 29th, 2023.</p> <p>Here are the details of the winter school:</p> <ul> <li>Location: Sophie Barat Hall B352, Songsim Global Campus, The Catholic University of Korea</li> <li>Time: Weekdays, 14:00</li> <li>TA: <a href="https://nslab-cuk.github.io/member/hbkim" rel="external nofollow noopener" target="_blank">Ho Beom Kim</a> </li> </ul> <hr> <h3 class="no_toc" id="programme-at-a-glance">Programme at a glance</h3> <details> <summary>Dec 18th, 2023: What is AI (and Robots) Ethics?</summary> <p align="center"><iframe src="https://www.slideshare.net/slideshow/embed_code/key/hMuZLcqL6UY7Ok?hostedIn=slideshare&amp;page=upload" width="90%" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""></iframe></p> </details> <details class="no_toc"> <summary>Dec 19th, 2023: AI (and Robots) Ethics Guidelines</summary> <p align="center"><iframe src="https://www.slideshare.net/slideshow/embed_code/key/4fLh3MZW0cAIx0?hostedIn=slideshare&amp;page=upload" width="90%" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""></iframe></p> </details> <details class="no_toc"> <summary>Dec 20th, 2023: Ethical Agents in Everyday Life</summary> <p align="center"><iframe src="https://www.slideshare.net/slideshow/embed_code/key/e1M5FxHiWgeS1H?hostedIn=slideshare&amp;page=upload" width="90%" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""></iframe></p> </details> <details class="no_toc"> <summary>Dec 21st, 2023: Ethical Issues of AI (and Robots): Law</summary> <p align="center"><iframe src="https://www.slideshare.net/slideshow/embed_code/key/KOrt2rHBH7YJqb?hostedIn=slideshare&amp;page=upload" width="90%" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""></iframe></p> </details> <details class="no_toc"> <summary>Dec 22nd, 2023: Ethical Issues of AI (and Robots): Art</summary> <p align="center"><iframe src="https://www.slideshare.net/slideshow/embed_code/key/ekkIuwFM3tPhdv?hostedIn=slideshare&amp;page=upload" width="90%" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""></iframe></p> </details> <details class="no_toc"> <summary>Dec 26th, 2023: Technology for Implementing Ethical AI (and Robots) I</summary> <p align="center"><iframe src="https://www.slideshare.net/slideshow/embed_code/key/9VtrvoCaVr4304?hostedIn=slideshare&amp;page=upload" width="90%" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""></iframe></p> </details> <details class="no_toc"> <summary>Dec 27th, 2023: Technology for Implementing Ethical AI (and Robots) II</summary> <p align="center"><iframe src="https://www.slideshare.net/slideshow/embed_code/key/kplgoQjZaBCSLs?hostedIn=slideshare&amp;page=upload" width="90%" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""></iframe></p> </details> <details class="no_toc"> <summary>Dec 28th, 2023: Biases in AI</summary> <p align="center"><iframe src="https://www.slideshare.net/slideshow/embed_code/key/hblzyt9msVGO23?hostedIn=slideshare&amp;page=upload" width="90%" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""></iframe></p> </details> <details class="no_toc"> <summary>Dec 29th, 2023: Applications of Ethical AI (and Robots): Medicine and Law</summary> <p align="center"><iframe src="https://www.slideshare.net/slideshow/embed_code/key/EcPphEBMgwCKVU?hostedIn=slideshare&amp;page=upload" width="90%" height="485" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px; margin-bottom:5px; max-width: 100%;" allowfullscreen=""></iframe></p> </details> <hr> <p align="center"><a href="https://nslab-cuk.github.io/" rel="external nofollow noopener" target="_blank"><img align="center" src="/images/Logo_Square.png" style="width : 120px; margin : 10px"></a><a href="https://cukai.catholic.ac.kr/cukai/index.html" rel="external nofollow noopener" target="_blank"><img align="center" src="/images/AI_Logo.png" style="width : 110px; margin : 15px"></a><a href="https://linc.catholic.ac.kr/lincplus/index.html" rel="external nofollow noopener" target="_blank"><img align="center" src="/images/CUKLINK_Logo.jpg" style="width : 380px; margin : 10px"></a></p> <hr> <p class="post-meta"> 1 min read Â  Â· Â  November 21, 2023 Â  Â· Â  Network Science Lab @ CUK </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="https://nslab-cuk.github.io/2023/08/30/LiteralKG/" target="_blank" rel="external nofollow noopener">LiteralKG, A Novel GNN Model for Learning Literal-aware Representations of Medical Knowledge Graphs</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <p>We present <a href="https://github.com/NSLab-CUK/LiteralKG" rel="external nofollow noopener" target="_blank">LiteralKG</a>, a novel GNN model for learning literal-aware representations of medical knowledge graphs, which can integrate different types of literal information and graph structural features into unified vector representations. LiteralKG is developed by NS Lab, CUK, based on pure PyTorch backend.</p> <p align="center"> <img src="/images/LiteralKG.jpg" alt="LiteralKG Architecture" width="800"> <br> <b></b> The overall architecture of LiteralKG. </p> <p>Over the past few years, Knowledge Graph (KG) embedding has been used to benefit the diagnosis of animal diseases by analyzing electronic medical records (EMRs), such as notes and veterinary records. However, learning representations to capture entities and relations with literal information in KGs is challenging as the KGs show heterogeneous properties and various types of literal information. Meanwhile, the existing methods mostly aim to preserve graph structures surrounding target nodes without considering different types of literals, which could also carry significant information. We propose <strong>LiteralKG</strong>, a knowledge graph embedding model for efficiently diagnosing animal diseases, which could learn various types of literal information and graph structure and fuse them into unified representations. Specifically, we construct a knowledge graph that is built from EMRs along with literal information collected from various animal hospitals. We then fuse different types of entities and node feature information into unified vector representations through gate networks. Finally, we propose a self-supervised learning task to learn graph structure in pretext tasks and then towards various downstream tasks. Experimental results on link prediction tasks demonstrate that our model outperforms the baselines that consist of state-of-the-art models.</p> <h2 id="a-short-description-of-literalkg">A short description of <strong>LiteralKG</strong>:</h2> <ul> <li>We construct a medical knowledge graph that comprises 595,172 entities and 16 relation types from various EMRs.</li> <li> <strong>LiteralKG</strong> could learn different types of literal information and graph structure and then fuse them into unified representations.</li> <li> <strong>LiteralKG</strong>, a self-supervised learning framework for Knowledge Graph, that could learn the graph structure from pretext tasks to generate representations, and then the pre-trained model is used for downstream tasks to predict animal diseases.</li> <li>The experimental results on the KG with different types of GNN aggregators and residual connection and identity mapping show the superiority of <strong>LiteralKG</strong> over baselines.</li> </ul> <h2 id="the-literalkg-is-available-at">The <strong>LiteralKG</strong> is available at:</h2> <ul> <li><a href="https://github.com/NSLab-CUK/LiteralKG" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/badge/GitHub-Data%20&amp;%20Code-9B9B9B?style=flat-square&amp;logo=GitHub" alt="GitHub"></a></li> <li><a href="https://doi.org/10.1109/ACCESS.2023.3324046" rel="external nofollow noopener" target="_blank"><img src="http://img.shields.io/:DOI-10.1109/ACCESS.2023.3324046-FAB70C?style=flat-square&amp;logo=doi" alt="DOI"></a></li> <li><a href="https://arxiv.org/abs/2309.03219" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/badge/arXiv-2309.03219-b31b1b?style=flat-square&amp;logo=arxiv&amp;logoColor=red" alt="arXiv"></a></li> <li><a href="https://paperswithcode.com/paper/companion-animal-disease-diagnostics-based-on" rel="external nofollow noopener" target="_blank"><img src="https://custom-icon-badges.demolab.com/badge/Papers%20With%20Code-LiteralKG-21CBCE?style=flat-square&amp;logo=paperswithcode" alt="PWC"></a></li> </ul> <h2 id="cite-literalkg-as">Cite â€œ<strong>LiteralKG</strong>â€ as:</h2> <p>Please cite our <a href="https://ieeexplore.ieee.org/abstract/document/10283810" rel="external nofollow noopener" target="_blank">paper</a> if you find <em>LiteralKG</em> useful in your work:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@Article{Hoang2023,
  author  = {Van Thuy Hoang and Thanh Sang Nguyen and Sangmyeong Lee and Jooho Lee and Luong Vuong Nguyen and O-Joun Lee},
  title   = {Companion Animal Disease Diagnostics Based on Literal-Aware Medical Knowledge Graph Representation Learning},
  journal = {IEEE Access},
  year    = {2023},
  volume  = {11},
  pages   = {114238--114249},
  month   = oct,
  issn    = {2169-3536},
  doi     = {10.1109/ACCESS.2023.3324046},
}
</code></pre></div></div> <p><img class="emoji" title=":page_facing_up:" alt=":page_facing_up:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c4.png" height="20" width="20"><img class="emoji" title=":woman_technologist:" alt=":woman_technologist:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f469-1f4bb.png" height="20" width="20"><img class="emoji" title=":bookmark_tabs:" alt=":bookmark_tabs:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d1.png" height="20" width="20"><img class="emoji" title=":label:" alt=":label:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3f7.png" height="20" width="20"><img class="emoji" title=":black_nib:" alt=":black_nib:" src="https://github.githubassets.com/images/icons/emoji/unicode/2712.png" height="20" width="20"></p> <h2 id="contributors">Contributors:</h2> <p><a href="https://github.com/NSLab-CUK/LiteralKG/graphs/contributors" rel="external nofollow noopener" target="_blank"> <img src="https://contrib.rocks/image?repo=NSLab-CUK/LiteralKG"> </a></p> <hr> <p><a href="https://nslab-cuk.github.io/" rel="external nofollow noopener" target="_blank"><img src="https://github.com/NSLab-CUK/NSLab-CUK/raw/main/Logo_Dual_Wide.png"></a></p> <hr> <p class="post-meta"> 1 min read Â  Â· Â  August 30, 2023 Â  Â· Â  Network Science Lab @ CUK </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="https://nslab-cuk.github.io/2023/08/17/UGT/" target="_blank" rel="external nofollow noopener">UGT, A Novel Graph Transformer Model for Unifying Local and Global Graph Structural Features</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <p>We present <a href="https://github.com/NSLab-CUK/Unified-Graph-Transformer" rel="external nofollow noopener" target="_blank">UGT</a>, a novel Graph Transformer model specialised in preserving both local and global graph structures and developed by NS Lab, CUK based on pure PyTorch backend.</p> <p align="center"> <img src="/images/UGT.jpg" alt="Graph Transformer Architecture" width="800"> <br> <b></b> The overall architecture of Unified Graph Transformer Networks. </p> <p>Over the past few years, graph neural networks and graph transformers have been successfully used to analyze graph-structured data, mainly focusing on node classification and link prediction tasks. However, the existing studies mostly only consider local connectivity while ignoring long-range connectivity and the roles of nodes. We propose Unified Graph Transformer Networks (UGT) that effectively integrate local and global structural information into fixed-length vector representations. UGT learns local structure by identifying the local substructures and aggregating features of the k-hop neighborhoods of each node. We construct virtual edges, bridging distant nodes with structural similarity to capture the long-range dependencies. UGT learns unified representations through self-attention, encoding structural distance and p-step transition probability between node pairs. Furthermore, we propose a self-supervised learning task that effectively learns transition probability to fuse local and global structural features, which could then be transferred to other downstream tasks. Experimental results on real-world benchmark datasets over various downstream tasks showed that UGT significantly outperformed baselines that consist of state-of-the-art models. In addition, UGT reaches the third-order Weisfeiler-Lehman power to distinguish non-isomorphic graph pairs.</p> <h2 id="a-short-description-of-ugt">A short description of UGT:</h2> <ul> <li>UGT could learn both local and global structural information and fuse them into unified representations.</li> <li>UGT capture the long-range dependencies between distant nodes as long as they are structurally similar. To do that, UGT constructs virtual edges, bridging the distant nodes with structural similarity.</li> <li>UGT captures local structure through structural identity in addressing local non-isomorphic substructures and finding similar ones.</li> <li>UGT, a self-supervised learning framework for graphs, that bridges the conceptual gap between local and global structural features by preserving transition probabilities between nodes in multi-scales.</li> <li>Experiments on fifteen publicly available datasets over various downstream tasks demonstrate the superiority of UGT over SOTA baselines.</li> <li>UGT reaches the third-order Weisfeiler-Lehman (3d-WL) power to distinguish non-isomorphic graph pairs.</li> </ul> <h2 id="the-ugt-is-available-at">The UGT is available at:</h2> <ul> <li><a href="https://doi.org/10.1609/aaai.v38i11.29138" rel="external nofollow noopener" target="_blank"><img src="http://img.shields.io/:DOI-10.1609/aaai.v38i11.29138-FAB70C?style=flat-square&amp;logo=doi" alt="DOI"></a></li> <li><a href="https://github.com/NSLab-CUK/Unified-Graph-Transformer" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/badge/GitHub-Data%20&amp;%20Code-9B9B9B?style=flat-square&amp;logo=GitHub" alt="GitHub"></a></li> <li><a href="https://arxiv.org/abs/2308.09517" rel="external nofollow noopener" target="_blank"><img src="https://img.shields.io/badge/arXiv-2308.09517-b31b1b?style=flat-square&amp;logo=arxiv&amp;logoColor=red" alt="arXiv"></a></li> <li><a href="https://paperswithcode.com/paper/transitivity-preserving-graph-representation" rel="external nofollow noopener" target="_blank"><img src="https://custom-icon-badges.demolab.com/badge/Papers%20With%20Code-UGT-21CBCE?style=flat-square&amp;logo=paperswithcode" alt="PwC"></a></li> </ul> <h2 id="cite-ugt-as">Cite â€œUGTâ€ as:</h2> <p>Please cite our <a href="https://arxiv.org/abs/2308.09517" rel="external nofollow noopener" target="_blank">paper</a> if you find <em>UGT</em> useful in your work:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{Hoang_2024,
  author    = {Hoang, Van Thuy and Lee, O-Joun},
  booktitle = {Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI 2024)},
  title     = {Transitivity-Preserving Graph Representation Learning for Bridging Local Connectivity and Role-Based Similarity},
  year      = {2024},
  address   = {Vancouver, Canada},
  editor    = {Michael Wooldridge and Jennifer Dy and Sriraam Natarajan},
  month     = feb,
  number    = {11},
  pages     = {12456--12465},
  publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
  volume    = {38},
  doi       = {10.1609/aaai.v38i11.29138},
  issn      = {2159-5399},
  url       = {https://doi.org/10.1609/aaai.v38i11.29138},
}
</code></pre></div></div> <p><img class="emoji" title=":page_facing_up:" alt=":page_facing_up:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4c4.png" height="20" width="20"><img class="emoji" title=":woman_technologist:" alt=":woman_technologist:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f469-1f4bb.png" height="20" width="20"><img class="emoji" title=":bookmark_tabs:" alt=":bookmark_tabs:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4d1.png" height="20" width="20"><img class="emoji" title=":label:" alt=":label:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f3f7.png" height="20" width="20"><img class="emoji" title=":black_nib:" alt=":black_nib:" src="https://github.githubassets.com/images/icons/emoji/unicode/2712.png" height="20" width="20"></p> <h2 id="contributors">Contributors:</h2> <p><a href="https://github.com/NSLab-CUK/Unified-Graph-Transformer/graphs/contributors" rel="external nofollow noopener" target="_blank"> <img src="https://contrib.rocks/image?repo=NSLab-CUK/Unified-Graph-Transformer"> </a></p> <hr> <p><a href="https://nslab-cuk.github.io/" rel="external nofollow noopener" target="_blank"><img src="https://github.com/NSLab-CUK/NSLab-CUK/raw/main/Logo_Dual_Wide.png"></a></p> <hr> <p class="post-meta"> 1 min read Â  Â· Â  August 17, 2023 Â  Â· Â  Network Science Lab @ CUK </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="https://nslab-cuk.github.io/2023/08/08/Seminar/" target="_blank" rel="external nofollow noopener">2023 CUK-AICT Joint Seminar on AI</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote> <p>The 2023 CUK-AICT Joint Seminar on Artificial Intelligence</p> </blockquote> <blockquote> <p>(2023ë…„ ê°€í†¨ë¦­ëŒ€í•™êµ-ì°¨ì„¸ëŒ€ìœµí•©ê¸°ìˆ ì—°êµ¬ì› ê³µë™ ì¸ê³µì§€ëŠ¥ ì„¸ë¯¸ë‚˜)</p> </blockquote> <hr> <p>NS Lab@CUK and the Advanced Institute of Convergence Technology (AICT) are organizing a joint seminar focusing on diverse application domains of artificial intelligence. The seminar series features 5 presentations by AICT researchers and is scheduled for August 22nd, 2023.</p> <p>Here are the details of the joint seminar:</p> <ul> <li>Location: Seminar Hall I, Advanced Institute of Convergence Technology, 145, Gwanggyo-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, Republic of Korea</li> <li>Date &amp; Time: August 22nd, 2023, Tuesday, 09:30 AM ğŸ•¤ (Departure from CUK at 08:30 AM ğŸ•£)</li> <li>Assistant: <a href="https://nslab-cuk.github.io/member/helee" rel="external nofollow noopener" target="_blank">Hyo Eun Lee</a> </li> </ul> <hr> <h3 id="programme-at-a-glance">Programme at a glance</h3> <ul> <li>10:00 â€“ 11:00 - Hyungjoo Kim, Senior Researcher (ê¹€í˜•ì£¼ ì„ ì„ì—°êµ¬ì›): â€œAI Applications in Autonomous Cooperative Driving: Current Research (ì¸ê³µì§€ëŠ¥ í™œìš© ììœ¨í˜‘ë ¥ì£¼í–‰ ì—°êµ¬ë™í–¥)â€</li> <li>11:00 â€“ 12:00 - SuJin Bak, Senior Researcher (ë°•ìˆ˜ì§„ ì„ ì„ì—°êµ¬ì›): â€œAI-powered Brain-Computer Interfaces: Current Research (ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ë‡Œ-ì»´í“¨í„° ì¸í„°í˜ì´ìŠ¤ ì—°êµ¬ë™í–¥)â€</li> <li>13:30 â€“ 14:30 - Gyuwon Song, Principal Researcher (ì†¡ê·œì› ì±…ì„ì—°êµ¬ì›): â€œLeveraging Data Science for Addressing Social Challenges (ì‚¬íšŒë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ë°ì´í„°ê³¼í•™ ì—°êµ¬ë™í–¥)â€</li> <li>14:30 â€“ 15:30 - Sungyeop Jung, Senior Researcher (ì •ì„±ì—½ ì„ ì„ì—°êµ¬ì›): â€œInnovations in Semiconductor Design through AI (ì¸ê³µì§€ëŠ¥ í™œìš© ë°˜ë„ì²´ ì„¤ê³„ ì—°êµ¬ë™í–¥)â€</li> <li>15:30 â€“ 16:30 - Lab Tour</li> </ul> <hr> <p align="center"><img src="/images/CUK-AICT-Seminar-23.jpg" style="width : 90%; max-width: 90%"></p> <hr> <p align="center"><a href="https://nslab-cuk.github.io/" rel="external nofollow noopener" target="_blank"><img align="center" src="/images/Logo_Square.png" style="width : 120px; margin : 10px"></a><a href="https://cukai.catholic.ac.kr/cukai/index.html" rel="external nofollow noopener" target="_blank"><img align="center" src="/images/AI_Logo.png" style="width : 110px; margin : 15px"></a><a href="https://linc.catholic.ac.kr/lincplus/index.html" rel="external nofollow noopener" target="_blank"><img align="center" src="/images/CUKLINK_Logo.jpg" style="width : 380px; margin : 10px"></a></p> <hr> <p class="post-meta"> 1 min read Â  Â· Â  August 08, 2023 Â  Â· Â  Network Science Lab @ CUK </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> </p> </li> <li> <h3> <a class="post-title" href="https://nslab-cuk.github.io/2023/08/06/Seminar/" target="_blank" rel="external nofollow noopener">2023 Fall CUK AI Seminar Series</a> <svg width="2rem" height="2rem" viewbox="0 0 40 40" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </h3> <p></p> <blockquote> <p>The 2023 Fall CUK Artificial Intelligence Seminar Series</p> </blockquote> <blockquote> <p>(2023ë…„ ê°€ì„ ê°€í†¨ë¦­ëŒ€í•™êµ ì¸ê³µì§€ëŠ¥ ì„¸ë¯¸ë‚˜)</p> </blockquote> <hr> <p>The Department of AI at CUK is hosting a seminar series focusing on diverse application domains of artificial intelligence. Spanning 15 weeks, from August 29th, 2023, to December 5th, 2023, this series will feature 13 talks by leading experts from both the corporate world and research institutions.</p> <p>Details of the seminar series are as follows:</p> <ul> <li>Location: Sophie Barat Hall B351, Songsim Global Campus, The Catholic University of Korea</li> <li>Time: Tuesdays, 18:00</li> <li>Assistant: <a href="https://nslab-cuk.github.io/member/helee" rel="external nofollow noopener" target="_blank">Hyo Eun Lee</a> </li> </ul> <hr> <h3 id="programme-at-a-glance">Programme at a glance</h3> <ul> <li>Aug 29th, 2023 - Donghwan Kwon, Klaytn Foundation (Klaytn ì¬ë‹¨ ê¶Œë™í™˜): â€œWhat Problems is Blockchain Aiming to Solve? (ë¸”ë¡ì²´ì¸ì´ í’€ê³ ì í•˜ëŠ” ë¬¸ì œëŠ” ë¬´ì—‡ì¸ê°€?)â€</li> <li>Sep 05th, 2023 - Sojung An, KIAPS (ì°¨ì„¸ëŒ€ìˆ˜ì¹˜ì˜ˆë³´ëª¨ë¸ê°œë°œì‚¬ì—…ë‹¨ ì•ˆì†Œì •): â€œUtilizing AI for Advanced Weather Forecasting and Climate Disaster Mitigation (ê¸°ìƒì˜ˆì¸¡ ë° ê¸°í›„ì¬ë‚œ ëŒ€ì‘ì„ ìœ„í•œ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ í™œìš©)â€</li> <li>Sep 12th, 2023 - Jin-Pyung Kim, Global Bridge (ê¸€ë¡œë²Œë¸Œë¦¿ì§€ ê¹€ì§„í‰): â€œThe Role of Deep Learning in Monitoring and Predicting Traffic Flow (êµí†µíë¦„ ëª¨ë‹ˆí„°ë§ ë° ì˜ˆì¸¡ì„ ìœ„í•œ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì˜ í™œìš©)â€</li> <li>Sep 19th, 2023 - Seulgi Moon, Seung Pyung Lawfirm (ë²•ë¬´ë²•ì¸ ìŠ¹í‰ ë¬¸ìŠ¬ê¸°): â€œThe Intersection of Law and Artificial Intelligence (ë²•ê³¼ AI)â€</li> <li>Sep 26th, 2023 - Yongjin Park, LG Display (LG ë””ìŠ¤í”Œë ˆì´ ë°•ìš©ì§„): â€œEmerging Trends of AI in Manufacturing (ì œì¡°ì—…ì—ì„œì˜ AI í™œìš© í˜„í™©)â€</li> <li>Oct 10th, 2023 - Keun-Tae Kim, KIST (í•œêµ­ê³¼í•™ê¸°ìˆ ì—°êµ¬ì› ê¹€ê·¼íƒœ): â€œSmart Interfaces Powered by Bio-signals: Moving the World with Thought (ìƒê°ëŒ€ë¡œ ì›€ì§ì´ëŠ” ì„¸ìƒ: ìƒì²´ì‹ í˜¸ ê¸°ë°˜ì˜ ì§€ëŠ¥í˜• ì¸í„°í˜ì´ìŠ¤ ê¸°ìˆ )â€</li> <li>Oct 24th, 2023 - Hajun Hwang, Republic of Korea Air Force (ê³µêµ° í™©í•˜ì¤€): â€œThe Rise of AI in Unmanned Aerial Vehicles (ë¬´ì¸ì „íˆ¬ê¸° ë¶„ì•¼ì—ì„œì˜ AI í™œìš© í˜„í™©)â€</li> <li>Oct 31st, 2023 - Jun-Yeong Heo, Samsung Electronics (ì‚¼ì„±ì „ì í—ˆì¤€ì˜): â€œAI Implementation in Samsung Electronicsâ€™ VD Division (ì‚¼ì„±ì „ì VDì‚¬ì—…ë¶€ì˜ AI í™œìš© í˜„í™©)â€</li> <li>Nov 07th, 2023 - Minhyeok Heo, NAVER LABS (ë„¤ì´ë²„ë©ìŠ¤ í—ˆë¯¼í˜): â€œArtificial Intelligence in Physical Spaces (ì˜¤í”„ë¼ì¸ ê³µê°„ì—ì„œì˜ ì¸ê³µì§€ëŠ¥ ê¸°ìˆ )â€</li> <li>Nov 14th, 2023 - Jae-Han Lee, Gauss Labs (ê°€ìš°ìŠ¤ë©ìŠ¤ ì´ì¬í•œ): â€œEvolving Research in Anomaly Detection and Localization for Industry Applications (ì‚°ì—… ë¶„ì•¼ì—ì„œ í™œìš©ë˜ëŠ” ì´ìƒ ê²€ì¶œ ë° ì§€ì—­í™” ê¸°ìˆ  ì—°êµ¬ ë™í–¥ê³¼ ì „ë§)â€</li> <li>Nov 21st, 2023 - Sangjin Kim, SK Hynix (SKí•˜ì´ë‹‰ìŠ¤ ê¹€ìƒì§„): â€œInnovations of AI in Semiconductor Manufacturing Processes (ë°˜ë„ì²´ ì œì¡° ê³µì •ì—ì„œì˜ AIí™œìš© ì‚¬ë¡€)â€</li> <li>Nov 28th, 2023 - Yong-Ho Jung, Danggeun Market (ë‹¹ê·¼ë§ˆì¼“ ì •ìš©í˜¸): â€œDanggeun Market: Revolutionizing Advertising with AI (ë‹¹ê·¼ë§ˆì¼“ê³¼ ê´‘ê³  AI)â€</li> <li>Dec 05th, 2023 - Do-Yeun Lee, Catholic Medical Center (ê°€í†¨ë¦­ì¤‘ì•™ì˜ë£Œì› ì´ë„ì—°): â€œNavigating Career Paths in the Hospital Sector (ë³‘ì› ê´€ë ¨ ì·¨ì—…ì„ ìœ„í•œ ì§„ë¡œ íƒìƒ‰)â€</li> </ul> <hr> <h3 id="past-events">Past events</h3> <ul> <li><a href="https://nslab-cuk.github.io/2023/08/06/Seminar/" rel="external nofollow noopener" target="_blank">The 2023 Fall CUK Artificial Intelligence Seminar Series</a></li> <li><a href="https://nslab-cuk.github.io/2023/02/21/Seminar/" rel="external nofollow noopener" target="_blank">The 2023 Spring CUK Artificial Intelligence Seminar Series</a></li> </ul> <hr> <p align="center"><img src="/images/2023-2-Seminar-Poster.webp" style="width : 90%; max-width: 90%"></p> <hr> <p align="center"><a href="https://ai.catholic.ac.kr/" rel="external nofollow noopener" target="_blank"><img align="center" src="/images/AI_Logo.png" style="width : 110px; margin : 15px"></a><a href="https://linc.catholic.ac.kr/lincplus/index.html" rel="external nofollow noopener" target="_blank"><img align="center" src="/images/CUKLINK_Logo.jpg" style="width : 380px; margin : 10px; max-width: 90%"></a></p> <hr> <p class="post-meta"> 1 min read Â  Â· Â  August 06, 2023 Â  Â· Â  Network Science Lab @ CUK </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a> </p> </li> </ul> <nav aria-label="Blog page naviation"> <ul class="pagination pagination-lg justify-content-center"> <li class="page-item "> <a class="page-link" href="/blog/page/4/" tabindex="-1" aria-disabled="4">Newer</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/2/index.html" title="blog - page 2">2</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/3/index.html" title="blog - page 3">3</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/4/index.html" title="blog - page 4">4</a> </li> <li class="page-item active"> <a class="page-link" href="/blog/page/5/index.html" title="blog - page 5">5</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/6/index.html" title="blog - page 6">6</a> </li> <li class="page-item "> <a class="page-link" href="/blog/page/6/">Older</a> </li> </ul> </nav> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> Â© Copyright 2025 O-Joun Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Affiliated with <a href="https://nslab-cuk.github.io/" target="_blank" rel="external nofollow noopener">Network Science Lab @ CUK</a>. Last updated: April 04, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?5d75c11f89cd96294bf5e6dd1ee1bb30"></script> <script defer src="/assets/js/common.js?fcfacfb8c6281f5e68d5a7d348186eb1"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>